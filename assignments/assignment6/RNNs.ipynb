{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRayBcqVwBgc",
    "colab_type": "text"
   },
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code",
    "id": "P59NYU98GCb9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69.0
    },
    "outputId": "7753ce37-db96-420e-deca-ba428e5213ec",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986783844E12,
     "user_tz": -180.0,
     "elapsed": 21999.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 16.0MB 3.4MB/s \n",
      "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 5.4MB 3.4MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !wget https://developer.nvidia.com/compute/cuda/9.2/Prod2/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.148-1_amd64\n",
    "# !dpkg -i cuda-repo-ubuntu1710-9-2-local_9.2.148-1_amd64\n",
    "# # You can use this line to find out the directory name\n",
    "# # !ls /var/ | grep cuda-repo\n",
    "# !apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
    "# !apt-get update\n",
    "# !apt-get install cuda\n",
    "# !pip3 -qq install torch==0.4.1\n",
    "!pip3 -qq install bokeh==0.13.0\n",
    "!pip3 -qq install gensim==3.6.0\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "8sVtGHmA9aBM",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code",
    "id": "TiA2dGmgF1rW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86.0
    },
    "outputId": "6093f22b-40bb-471f-84f0-54ed282fcb36",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986786533E12,
     "user_tz": -180.0,
     "elapsed": 24580.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code",
    "id": "QstS4NO0L97c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451.0
    },
    "outputId": "88fa3238-6997-4465-de33-e44995277d17",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986786549E12,
     "user_tz": -180.0,
     "elapsed": 24565.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code",
    "id": "xTai8Ta0lgwL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69.0
    },
    "outputId": "4edbdce2-aeea-48eb-a81a-ec6622d2be79",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986814664E12,
     "user_tz": -180.0,
     "elapsed": 52627.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "14c75ea9-8761-4c0f-b06d-6d6fbf87f762",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986814848E12,
     "user_tz": -180.0,
     "elapsed": 52781.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'ADP', 'DET', '.', 'ADV', 'NUM', 'X', 'PRON', 'NOUN', 'VERB', 'CONJ', 'PRT', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code",
    "id": "URC1B2nvPGFt",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323.0
    },
    "outputId": "f9bac5e9-75f4-4b92-a6b3-a98227848831",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986815688E12,
     "user_tz": -180.0,
     "elapsed": 53591.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAEyCAYAAABH+Yw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code",
    "id": "5rWmSToIaeAo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "a66c8c95-4f62-4403-9f4f-3e17c399dea3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986819275E12,
     "user_tz": -180.0,
     "elapsed": 57135.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "fc3f1ac7-f596-484a-bbe5-c60deb15ad20",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986825238E12,
     "user_tz": -180.0,
     "elapsed": 63071.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code",
    "id": "8XCuxEBVbOY_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "6b070970-7351-4756-df14-e6fd188c8c5b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986830956E12,
     "user_tz": -180.0,
     "elapsed": 68772.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "RtRbz1SwgEqc",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "DhsTKZalfih6",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code",
    "id": "l4XsRII5kW5x",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "53b76439-5d70-4bfa-ac05-22279003ed04",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557986832268E12,
     "user_tz": -180.0,
     "elapsed": 70016.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "WVEHju54d68T",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self.last_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        results = []\n",
    "        for sentence in inputs:\n",
    "            embeds = self.word_embeddings(sentence)\n",
    "            lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "            results.append(self.last_layer(lstm_out.view(len(sentence), -1)))\n",
    "        return torch.stack(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code",
    "id": "jbrxsZ2mehWB",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "759db36d-d9dd-4646-f855-964c3aa2d15d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.5579870268E12,
     "user_tz": -180.0,
     "elapsed": 468.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05434782608695652"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "def calc_accuracy(preds, gt):\n",
    "    real = (gt != 0).float()\n",
    "    correct = ((preds == gt).float() * real).sum().item()\n",
    "    total = real.sum().item()\n",
    "\n",
    "    return correct, total\n",
    "cor, tot = calc_accuracy(preds, y_batch)\n",
    "cor / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "GMUyUm1hgpe3",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "FprPQ0gllo7b",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = calc_accuracy(preds, y_batch)\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1754.0
    },
    "outputId": "6a806b8e-6c29-45dc-de49-806531f92298",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557994472987E12,
     "user_tz": -180.0,
     "elapsed": 5525013.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.74342, Accuracy = 75.77%: 100%|██████████| 572/572 [01:41<00:00,  5.64it/s]\n",
      "[1 / 50]   Val: Loss = 0.39294, Accuracy = 86.85%: 100%|██████████| 13/13 [00:09<00:00,  1.33it/s]\n",
      "[2 / 50] Train: Loss = 0.32210, Accuracy = 89.25%: 100%|██████████| 572/572 [01:41<00:00,  5.65it/s]\n",
      "[2 / 50]   Val: Loss = 0.26966, Accuracy = 90.84%: 100%|██████████| 13/13 [00:09<00:00,  1.67it/s]\n",
      "[3 / 50] Train: Loss = 0.23322, Accuracy = 92.00%: 100%|██████████| 572/572 [01:40<00:00,  5.49it/s]\n",
      "[3 / 50]   Val: Loss = 0.22401, Accuracy = 91.25%: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n",
      "[4 / 50] Train: Loss = 0.18975, Accuracy = 93.26%: 100%|██████████| 572/572 [01:41<00:00,  5.62it/s]\n",
      "[4 / 50]   Val: Loss = 0.20656, Accuracy = 91.75%: 100%|██████████| 13/13 [00:10<00:00,  1.46it/s]\n",
      "[5 / 50] Train: Loss = 0.16383, Accuracy = 93.97%: 100%|██████████| 572/572 [01:40<00:00,  5.70it/s]\n",
      "[5 / 50]   Val: Loss = 0.19034, Accuracy = 92.23%: 100%|██████████| 13/13 [00:09<00:00,  1.78it/s]\n",
      "[6 / 50] Train: Loss = 0.14644, Accuracy = 94.47%: 100%|██████████| 572/572 [01:40<00:00,  5.68it/s]\n",
      "[6 / 50]   Val: Loss = 0.18422, Accuracy = 92.52%: 100%|██████████| 13/13 [00:09<00:00,  1.75it/s]\n",
      "[7 / 50] Train: Loss = 0.13425, Accuracy = 94.78%: 100%|██████████| 572/572 [01:42<00:00,  5.55it/s]\n",
      "[7 / 50]   Val: Loss = 0.17468, Accuracy = 92.72%: 100%|██████████| 13/13 [00:08<00:00,  1.40it/s]\n",
      "[8 / 50] Train: Loss = 0.12556, Accuracy = 95.04%: 100%|██████████| 572/572 [01:40<00:00,  5.68it/s]\n",
      "[8 / 50]   Val: Loss = 0.17096, Accuracy = 92.83%: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n",
      "[9 / 50] Train: Loss = 0.11927, Accuracy = 95.19%: 100%|██████████| 572/572 [01:41<00:00,  5.66it/s]\n",
      "[9 / 50]   Val: Loss = 0.16863, Accuracy = 92.94%: 100%|██████████| 13/13 [00:09<00:00,  1.33it/s]\n",
      "[10 / 50] Train: Loss = 0.11458, Accuracy = 95.30%: 100%|██████████| 572/572 [01:41<00:00,  5.61it/s]\n",
      "[10 / 50]   Val: Loss = 0.16623, Accuracy = 93.44%: 100%|██████████| 13/13 [00:09<00:00,  1.28it/s]\n",
      "[11 / 50] Train: Loss = 0.11102, Accuracy = 95.42%: 100%|██████████| 572/572 [01:40<00:00,  5.68it/s]\n",
      "[11 / 50]   Val: Loss = 0.17095, Accuracy = 93.03%: 100%|██████████| 13/13 [00:09<00:00,  1.57it/s]\n",
      "[12 / 50] Train: Loss = 0.10835, Accuracy = 95.47%: 100%|██████████| 572/572 [01:42<00:00,  5.58it/s]\n",
      "[12 / 50]   Val: Loss = 0.17024, Accuracy = 92.94%: 100%|██████████| 13/13 [00:09<00:00,  1.84it/s]\n",
      "[13 / 50] Train: Loss = 0.10626, Accuracy = 95.51%: 100%|██████████| 572/572 [01:41<00:00,  5.65it/s]\n",
      "[13 / 50]   Val: Loss = 0.17032, Accuracy = 93.01%: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n",
      "[14 / 50] Train: Loss = 0.10459, Accuracy = 95.56%: 100%|██████████| 572/572 [01:40<00:00,  5.68it/s]\n",
      "[14 / 50]   Val: Loss = 0.16591, Accuracy = 93.04%: 100%|██████████| 13/13 [00:08<00:00,  1.44it/s]\n",
      "[15 / 50] Train: Loss = 0.10332, Accuracy = 95.58%: 100%|██████████| 572/572 [01:42<00:00,  5.57it/s]\n",
      "[15 / 50]   Val: Loss = 0.17284, Accuracy = 92.96%: 100%|██████████| 13/13 [00:09<00:00,  1.56it/s]\n",
      "[16 / 50] Train: Loss = 0.10210, Accuracy = 95.60%: 100%|██████████| 572/572 [01:39<00:00,  5.73it/s]\n",
      "[16 / 50]   Val: Loss = 0.16234, Accuracy = 93.87%: 100%|██████████| 13/13 [00:09<00:00,  1.68it/s]\n",
      "[17 / 50] Train: Loss = 0.10130, Accuracy = 95.62%: 100%|██████████| 572/572 [01:40<00:00,  5.71it/s]\n",
      "[17 / 50]   Val: Loss = 0.16474, Accuracy = 93.62%: 100%|██████████| 13/13 [00:09<00:00,  1.35it/s]\n",
      "[18 / 50] Train: Loss = 0.10068, Accuracy = 95.65%: 100%|██████████| 572/572 [01:42<00:00,  5.57it/s]\n",
      "[18 / 50]   Val: Loss = 0.16653, Accuracy = 93.73%: 100%|██████████| 13/13 [00:09<00:00,  1.40it/s]\n",
      "[19 / 50] Train: Loss = 0.09993, Accuracy = 95.66%: 100%|██████████| 572/572 [01:40<00:00,  5.70it/s]\n",
      "[19 / 50]   Val: Loss = 0.16627, Accuracy = 93.40%: 100%|██████████| 13/13 [00:09<00:00,  1.58it/s]\n",
      "[20 / 50] Train: Loss = 0.09939, Accuracy = 95.68%: 100%|██████████| 572/572 [01:40<00:00,  5.70it/s]\n",
      "[20 / 50]   Val: Loss = 0.16631, Accuracy = 93.68%: 100%|██████████| 13/13 [00:10<00:00,  1.41it/s]\n",
      "[21 / 50] Train: Loss = 0.09877, Accuracy = 95.69%: 100%|██████████| 572/572 [01:42<00:00,  5.59it/s]\n",
      "[21 / 50]   Val: Loss = 0.16233, Accuracy = 94.09%: 100%|██████████| 13/13 [00:09<00:00,  1.65it/s]\n",
      "[22 / 50] Train: Loss = 0.09852, Accuracy = 95.70%: 100%|██████████| 572/572 [01:40<00:00,  5.71it/s]\n",
      "[22 / 50]   Val: Loss = 0.17357, Accuracy = 93.09%: 100%|██████████| 13/13 [00:09<00:00,  1.37it/s]\n",
      "[23 / 50] Train: Loss = 0.09821, Accuracy = 95.71%: 100%|██████████| 572/572 [01:41<00:00,  5.61it/s]\n",
      "[23 / 50]   Val: Loss = 0.17298, Accuracy = 92.95%: 100%|██████████| 13/13 [00:09<00:00,  1.44it/s]\n",
      "[24 / 50] Train: Loss = 0.09792, Accuracy = 95.72%: 100%|██████████| 572/572 [01:40<00:00,  5.69it/s]\n",
      "[24 / 50]   Val: Loss = 0.16760, Accuracy = 93.64%: 100%|██████████| 13/13 [00:09<00:00,  1.68it/s]\n",
      "[25 / 50] Train: Loss = 0.09764, Accuracy = 95.72%: 100%|██████████| 572/572 [01:40<00:00,  5.71it/s]\n",
      "[25 / 50]   Val: Loss = 0.17456, Accuracy = 93.25%: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n",
      "[26 / 50] Train: Loss = 0.09734, Accuracy = 95.72%: 100%|██████████| 572/572 [01:41<00:00,  5.61it/s]\n",
      "[26 / 50]   Val: Loss = 0.17040, Accuracy = 93.20%: 100%|██████████| 13/13 [00:09<00:00,  1.83it/s]\n",
      "[27 / 50] Train: Loss = 0.09723, Accuracy = 95.71%: 100%|██████████| 572/572 [01:40<00:00,  5.68it/s]\n",
      "[27 / 50]   Val: Loss = 0.17246, Accuracy = 93.05%: 100%|██████████| 13/13 [00:09<00:00,  1.48it/s]\n",
      "[28 / 50] Train: Loss = 0.09715, Accuracy = 95.72%: 100%|██████████| 572/572 [01:40<00:00,  5.70it/s]\n",
      "[28 / 50]   Val: Loss = 0.16450, Accuracy = 94.09%: 100%|██████████| 13/13 [00:09<00:00,  1.35it/s]\n",
      "[29 / 50] Train: Loss = 0.09670, Accuracy = 95.74%: 100%|██████████| 572/572 [01:42<00:00,  5.58it/s]\n",
      "[29 / 50]   Val: Loss = 0.17365, Accuracy = 93.16%: 100%|██████████| 13/13 [00:09<00:00,  1.47it/s]\n",
      "[30 / 50] Train: Loss = 0.09648, Accuracy = 95.73%: 100%|██████████| 572/572 [01:40<00:00,  5.70it/s]\n",
      "[30 / 50]   Val: Loss = 0.16916, Accuracy = 93.50%: 100%|██████████| 13/13 [00:09<00:00,  1.56it/s]\n",
      "[31 / 50] Train: Loss = 0.09636, Accuracy = 95.76%: 100%|██████████| 572/572 [01:41<00:00,  5.63it/s]\n",
      "[31 / 50]   Val: Loss = 0.17744, Accuracy = 92.81%: 100%|██████████| 13/13 [00:09<00:00,  1.60it/s]\n",
      "[32 / 50] Train: Loss = 0.09623, Accuracy = 95.75%: 100%|██████████| 572/572 [01:41<00:00,  5.62it/s]\n",
      "[32 / 50]   Val: Loss = 0.17706, Accuracy = 92.77%: 100%|██████████| 13/13 [00:09<00:00,  1.74it/s]\n",
      "[33 / 50] Train: Loss = 0.09604, Accuracy = 95.77%: 100%|██████████| 572/572 [01:40<00:00,  5.69it/s]\n",
      "[33 / 50]   Val: Loss = 0.17334, Accuracy = 93.29%: 100%|██████████| 13/13 [00:08<00:00,  1.48it/s]\n",
      "[34 / 50] Train: Loss = 0.09590, Accuracy = 95.78%: 100%|██████████| 572/572 [01:41<00:00,  5.64it/s]\n",
      "[34 / 50]   Val: Loss = 0.17027, Accuracy = 93.57%: 100%|██████████| 13/13 [00:09<00:00,  1.77it/s]\n",
      "[35 / 50] Train: Loss = 0.09578, Accuracy = 95.76%: 100%|██████████| 572/572 [01:42<00:00,  5.58it/s]\n",
      "[35 / 50]   Val: Loss = 0.17029, Accuracy = 93.49%: 100%|██████████| 13/13 [00:08<00:00,  1.67it/s]\n",
      "[36 / 50] Train: Loss = 0.09577, Accuracy = 95.76%: 100%|██████████| 572/572 [01:39<00:00,  5.75it/s]\n",
      "[36 / 50]   Val: Loss = 0.18642, Accuracy = 92.99%: 100%|██████████| 13/13 [00:09<00:00,  1.37it/s]\n",
      "[37 / 50] Train: Loss = 0.09563, Accuracy = 95.77%: 100%|██████████| 572/572 [01:39<00:00,  5.73it/s]\n",
      "[37 / 50]   Val: Loss = 0.17331, Accuracy = 93.43%: 100%|██████████| 13/13 [00:09<00:00,  1.33it/s]\n",
      "[38 / 50] Train: Loss = 0.09553, Accuracy = 95.78%: 100%|██████████| 572/572 [01:40<00:00,  5.68it/s]\n",
      "[38 / 50]   Val: Loss = 0.17284, Accuracy = 93.03%: 100%|██████████| 13/13 [00:09<00:00,  1.37it/s]\n",
      "[39 / 50] Train: Loss = 0.09533, Accuracy = 95.78%: 100%|██████████| 572/572 [01:41<00:00,  5.65it/s]\n",
      "[39 / 50]   Val: Loss = 0.16917, Accuracy = 93.44%: 100%|██████████| 13/13 [00:08<00:00,  1.42it/s]\n",
      "[40 / 50] Train: Loss = 0.09524, Accuracy = 95.77%: 100%|██████████| 572/572 [01:42<00:00,  5.59it/s]\n",
      "[40 / 50]   Val: Loss = 0.16965, Accuracy = 93.55%: 100%|██████████| 13/13 [00:09<00:00,  1.76it/s]\n",
      "[41 / 50] Train: Loss = 0.09508, Accuracy = 95.76%: 100%|██████████| 572/572 [01:40<00:00,  5.69it/s]\n",
      "[41 / 50]   Val: Loss = 0.17511, Accuracy = 93.09%: 100%|██████████| 13/13 [00:09<00:00,  1.55it/s]\n",
      "[42 / 50] Train: Loss = 0.09498, Accuracy = 95.79%: 100%|██████████| 572/572 [01:41<00:00,  5.64it/s]\n",
      "[42 / 50]   Val: Loss = 0.16743, Accuracy = 93.41%: 100%|██████████| 13/13 [00:09<00:00,  1.71it/s]\n",
      "[43 / 50] Train: Loss = 0.09481, Accuracy = 95.79%: 100%|██████████| 572/572 [01:41<00:00,  5.62it/s]\n",
      "[43 / 50]   Val: Loss = 0.17272, Accuracy = 93.29%: 100%|██████████| 13/13 [00:09<00:00,  1.64it/s]\n",
      "[44 / 50] Train: Loss = 0.09497, Accuracy = 95.79%: 100%|██████████| 572/572 [01:40<00:00,  5.70it/s]\n",
      "[44 / 50]   Val: Loss = 0.16563, Accuracy = 93.82%: 100%|██████████| 13/13 [00:10<00:00,  1.29it/s]\n",
      "[45 / 50] Train: Loss = 0.09493, Accuracy = 95.80%: 100%|██████████| 572/572 [01:40<00:00,  5.67it/s]\n",
      "[45 / 50]   Val: Loss = 0.16641, Accuracy = 93.89%: 100%|██████████| 13/13 [00:09<00:00,  1.45it/s]\n",
      "[46 / 50] Train: Loss = 0.09467, Accuracy = 95.81%: 100%|██████████| 572/572 [01:40<00:00,  5.67it/s]\n",
      "[46 / 50]   Val: Loss = 0.16602, Accuracy = 93.78%: 100%|██████████| 13/13 [00:08<00:00,  1.91it/s]\n",
      "[47 / 50] Train: Loss = 0.09465, Accuracy = 95.80%: 100%|██████████| 572/572 [01:40<00:00,  5.68it/s]\n",
      "[47 / 50]   Val: Loss = 0.17072, Accuracy = 93.31%: 100%|██████████| 13/13 [00:09<00:00,  1.52it/s]\n",
      "[48 / 50] Train: Loss = 0.09448, Accuracy = 95.82%: 100%|██████████| 572/572 [01:40<00:00,  5.67it/s]\n",
      "[48 / 50]   Val: Loss = 0.17425, Accuracy = 93.36%: 100%|██████████| 13/13 [00:09<00:00,  1.66it/s]\n",
      "[49 / 50] Train: Loss = 0.09441, Accuracy = 95.81%: 100%|██████████| 572/572 [01:41<00:00,  5.61it/s]\n",
      "[49 / 50]   Val: Loss = 0.17808, Accuracy = 93.34%: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s]\n",
      "[50 / 50] Train: Loss = 0.09441, Accuracy = 95.81%: 100%|██████████| 572/572 [01:41<00:00,  5.65it/s]\n",
      "[50 / 50]   Val: Loss = 0.17468, Accuracy = 93.34%: 100%|██████████| 13/13 [00:09<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab_type": "code",
    "id": "98wr38_rw55D",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "8acba77f-0f30-466f-c97a-e4f5017c1354",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557994700981E12,
     "user_tz": -180.0,
     "elapsed": 18876.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LSTM tagger: 93.39%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct_count = 0\n",
    "sum_count = 0\n",
    "for X_batch_test, y_batch_test in iterate_batches((X_test, y_test), 64):\n",
    "    X_batch_test, y_batch_test = LongTensor(X_batch_test), LongTensor(y_batch_test)\n",
    "    logits = model(X_batch_test)\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    cur_correct_count, cur_sum_count = calc_accuracy(preds, y_batch_test)\n",
    "    correct_count += cur_correct_count\n",
    "    sum_count += cur_sum_count\n",
    "print(\"Accuracy of LSTM tagger: {:.2%}\".format( correct_count / sum_count ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "9nE3C3XUwBoR",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "660790e8-19e2-4d54-d2c3-2733736fbb2c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557994902105E12,
     "user_tz": -180.0,
     "elapsed": 73212.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab_type": "code",
    "id": "VsCstxiO03oT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "087eaf2b-c46b-4951-8882-ac18f32af281",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.557994930212E12,
     "user_tz": -180.0,
     "elapsed": 677.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "LxaRBpQd0pat",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(FloatTensor(embeddings))\n",
    "        self.lstm = nn.LSTM(self.word_embeddings.embedding_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
    "        self.last_layer = nn.Linear(lstm_hidden_dim* 2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        results = []\n",
    "        for sentence in inputs:\n",
    "            embeds = self.word_embeddings(sentence)\n",
    "            lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "            results.append(self.last_layer(lstm_out.view(len(sentence), -1)))\n",
    "        return torch.stack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1754.0
    },
    "outputId": "afe37081-7cba-4c3e-a14e-45485504130d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.55800911451E12,
     "user_tz": -180.0,
     "elapsed": 878391.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.66514, Accuracy = 80.34%: 100%|██████████| 572/572 [02:51<00:00,  3.34it/s]\n",
      "[1 / 50]   Val: Loss = 0.35334, Accuracy = 89.33%: 100%|██████████| 13/13 [00:18<00:00,  1.34s/it]\n",
      "[2 / 50] Train: Loss = 0.28409, Accuracy = 91.16%: 100%|██████████| 572/572 [02:53<00:00,  3.30it/s]\n",
      "[2 / 50]   Val: Loss = 0.26803, Accuracy = 91.41%: 100%|██████████| 13/13 [00:17<00:00,  1.27s/it]\n",
      "[3 / 50] Train: Loss = 0.22663, Accuracy = 92.65%: 100%|██████████| 572/572 [02:51<00:00,  3.34it/s]\n",
      "[3 / 50]   Val: Loss = 0.23315, Accuracy = 92.27%: 100%|██████████| 13/13 [00:17<00:00,  1.41s/it]\n",
      "[4 / 50] Train: Loss = 0.19997, Accuracy = 93.26%: 100%|██████████| 572/572 [02:52<00:00,  3.32it/s]\n",
      "[4 / 50]   Val: Loss = 0.21428, Accuracy = 92.65%: 100%|██████████| 13/13 [00:18<00:00,  1.29s/it]\n",
      "[5 / 50] Train: Loss = 0.18507, Accuracy = 93.62%: 100%|██████████| 572/572 [02:53<00:00,  3.30it/s]\n",
      "[5 / 50]   Val: Loss = 0.20666, Accuracy = 92.96%: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]\n",
      "[6 / 50] Train: Loss = 0.17581, Accuracy = 93.81%: 100%|██████████| 572/572 [02:51<00:00,  3.34it/s]\n",
      "[6 / 50]   Val: Loss = 0.19733, Accuracy = 93.18%: 100%|██████████| 13/13 [00:18<00:00,  1.29s/it]\n",
      "[7 / 50] Train: Loss = 0.16895, Accuracy = 93.94%: 100%|██████████| 572/572 [02:51<00:00,  3.33it/s]\n",
      "[7 / 50]   Val: Loss = 0.19748, Accuracy = 92.92%: 100%|██████████| 13/13 [00:18<00:00,  1.37s/it]\n",
      "[8 / 50] Train: Loss = 0.16407, Accuracy = 94.04%: 100%|██████████| 572/572 [02:50<00:00,  3.36it/s]\n",
      "[8 / 50]   Val: Loss = 0.18889, Accuracy = 93.16%: 100%|██████████| 13/13 [00:18<00:00,  1.30s/it]\n",
      "[9 / 50] Train: Loss = 0.16084, Accuracy = 94.11%: 100%|██████████| 572/572 [02:52<00:00,  4.06it/s]\n",
      "[9 / 50]   Val: Loss = 0.18590, Accuracy = 93.38%: 100%|██████████| 13/13 [00:18<00:00,  1.17s/it]\n",
      "[10 / 50] Train: Loss = 0.15751, Accuracy = 94.15%: 100%|██████████| 572/572 [02:53<00:00,  4.03it/s]\n",
      "[10 / 50]   Val: Loss = 0.18579, Accuracy = 93.37%: 100%|██████████| 13/13 [00:18<00:00,  1.20s/it]\n",
      "[11 / 50] Train: Loss = 0.15523, Accuracy = 94.23%: 100%|██████████| 572/572 [02:50<00:00,  3.35it/s]\n",
      "[11 / 50]   Val: Loss = 0.18264, Accuracy = 93.42%: 100%|██████████| 13/13 [00:18<00:00,  1.26s/it]\n",
      "[12 / 50] Train: Loss = 0.15298, Accuracy = 94.26%: 100%|██████████| 572/572 [02:51<00:00,  3.34it/s]\n",
      "[12 / 50]   Val: Loss = 0.18017, Accuracy = 93.42%: 100%|██████████| 13/13 [00:16<00:00,  1.10s/it]\n",
      "[13 / 50] Train: Loss = 0.15096, Accuracy = 94.31%: 100%|██████████| 572/572 [02:50<00:00,  3.35it/s]\n",
      "[13 / 50]   Val: Loss = 0.18090, Accuracy = 93.43%: 100%|██████████| 13/13 [00:16<00:00,  1.54s/it]\n",
      "[14 / 50] Train: Loss = 0.14961, Accuracy = 94.31%: 100%|██████████| 572/572 [02:51<00:00,  4.66it/s]\n",
      "[14 / 50]   Val: Loss = 0.17890, Accuracy = 93.48%: 100%|██████████| 13/13 [00:18<00:00,  1.12s/it]\n",
      "[15 / 50] Train: Loss = 0.14832, Accuracy = 94.35%: 100%|██████████| 572/572 [02:52<00:00,  4.55it/s]\n",
      "[15 / 50]   Val: Loss = 0.17779, Accuracy = 93.50%: 100%|██████████| 13/13 [00:18<00:00,  1.42s/it]\n",
      "[16 / 50] Train: Loss = 0.14681, Accuracy = 94.37%: 100%|██████████| 572/572 [02:51<00:00,  3.33it/s]\n",
      "[16 / 50]   Val: Loss = 0.17824, Accuracy = 93.48%: 100%|██████████| 13/13 [00:18<00:00,  1.35s/it]\n",
      "[17 / 50] Train: Loss = 0.14583, Accuracy = 94.40%: 100%|██████████| 572/572 [02:51<00:00,  3.34it/s]\n",
      "[17 / 50]   Val: Loss = 0.17613, Accuracy = 93.49%: 100%|██████████| 13/13 [00:17<00:00,  1.18s/it]\n",
      "[18 / 50] Train: Loss = 0.14476, Accuracy = 94.41%: 100%|██████████| 572/572 [02:51<00:00,  3.34it/s]\n",
      "[18 / 50]   Val: Loss = 0.17686, Accuracy = 93.47%: 100%|██████████| 13/13 [00:19<00:00,  1.17s/it]\n",
      "[19 / 50] Train: Loss = 0.14390, Accuracy = 94.42%: 100%|██████████| 572/572 [02:51<00:00,  3.34it/s]\n",
      "[19 / 50]   Val: Loss = 0.17648, Accuracy = 93.42%: 100%|██████████| 13/13 [00:18<00:00,  1.27s/it]\n",
      "[20 / 50] Train: Loss = 0.14278, Accuracy = 94.47%: 100%|██████████| 572/572 [02:52<00:00,  3.31it/s]\n",
      "[20 / 50]   Val: Loss = 0.17535, Accuracy = 93.50%: 100%|██████████| 13/13 [00:18<00:00,  1.25s/it]\n",
      "[21 / 50] Train: Loss = 0.14223, Accuracy = 94.45%: 100%|██████████| 572/572 [02:51<00:00,  3.33it/s]\n",
      "[21 / 50]   Val: Loss = 0.17631, Accuracy = 93.34%: 100%|██████████| 13/13 [00:19<00:00,  1.46s/it]\n",
      "[22 / 50] Train: Loss = 0.14168, Accuracy = 94.48%: 100%|██████████| 572/572 [02:52<00:00,  3.31it/s]\n",
      "[22 / 50]   Val: Loss = 0.17411, Accuracy = 93.51%: 100%|██████████| 13/13 [00:19<00:00,  1.27s/it]\n",
      "[23 / 50] Train: Loss = 0.14085, Accuracy = 94.50%: 100%|██████████| 572/572 [02:51<00:00,  4.01it/s]\n",
      "[23 / 50]   Val: Loss = 0.17206, Accuracy = 93.59%: 100%|██████████| 13/13 [00:18<00:00,  1.21s/it]\n",
      "[24 / 50] Train: Loss = 0.14028, Accuracy = 94.51%: 100%|██████████| 572/572 [02:52<00:00,  3.32it/s]\n",
      "[24 / 50]   Val: Loss = 0.17274, Accuracy = 93.48%: 100%|██████████| 13/13 [00:17<00:00,  1.13s/it]\n",
      "[25 / 50] Train: Loss = 0.13984, Accuracy = 94.51%: 100%|██████████| 572/572 [02:51<00:00,  3.33it/s]\n",
      "[25 / 50]   Val: Loss = 0.17539, Accuracy = 93.48%: 100%|██████████| 13/13 [00:18<00:00,  1.14s/it]\n",
      "[26 / 50] Train: Loss = 0.13912, Accuracy = 94.52%: 100%|██████████| 572/572 [02:54<00:00,  3.28it/s]\n",
      "[26 / 50]   Val: Loss = 0.17302, Accuracy = 93.59%: 100%|██████████| 13/13 [00:19<00:00,  1.34s/it]\n",
      "[27 / 50] Train: Loss = 0.13887, Accuracy = 94.54%: 100%|██████████| 572/572 [02:54<00:00,  3.28it/s]\n",
      "[27 / 50]   Val: Loss = 0.17269, Accuracy = 93.36%: 100%|██████████| 13/13 [00:17<00:00,  1.39s/it]\n",
      "[28 / 50] Train: Loss = 0.13821, Accuracy = 94.53%: 100%|██████████| 572/572 [02:53<00:00,  3.29it/s]\n",
      "[28 / 50]   Val: Loss = 0.17162, Accuracy = 93.65%: 100%|██████████| 13/13 [00:17<00:00,  1.22s/it]\n",
      "[29 / 50] Train: Loss = 0.13745, Accuracy = 94.57%: 100%|██████████| 572/572 [02:52<00:00,  3.31it/s]\n",
      "[29 / 50]   Val: Loss = 0.17314, Accuracy = 93.50%: 100%|██████████| 13/13 [00:19<00:00,  1.41s/it]\n",
      "[30 / 50] Train: Loss = 0.13723, Accuracy = 94.55%: 100%|██████████| 572/572 [02:52<00:00,  3.31it/s]\n",
      "[30 / 50]   Val: Loss = 0.17195, Accuracy = 93.53%: 100%|██████████| 13/13 [00:18<00:00,  1.45s/it]\n",
      "[31 / 50] Train: Loss = 0.13674, Accuracy = 94.58%: 100%|██████████| 572/572 [02:52<00:00,  3.32it/s]\n",
      "[31 / 50]   Val: Loss = 0.17113, Accuracy = 93.58%: 100%|██████████| 13/13 [00:19<00:00,  1.35s/it]\n",
      "[32 / 50] Train: Loss = 0.13646, Accuracy = 94.58%: 100%|██████████| 572/572 [02:54<00:00,  3.79it/s]\n",
      "[32 / 50]   Val: Loss = 0.17073, Accuracy = 93.56%: 100%|██████████| 13/13 [00:18<00:00,  1.54s/it]\n",
      "[33 / 50] Train: Loss = 0.13582, Accuracy = 94.59%: 100%|██████████| 572/572 [02:53<00:00,  3.29it/s]\n",
      "[33 / 50]   Val: Loss = 0.17113, Accuracy = 93.51%: 100%|██████████| 13/13 [00:18<00:00,  1.17s/it]\n",
      "[34 / 50] Train: Loss = 0.13582, Accuracy = 94.58%: 100%|██████████| 572/572 [02:52<00:00,  3.32it/s]\n",
      "[34 / 50]   Val: Loss = 0.17035, Accuracy = 93.59%: 100%|██████████| 13/13 [00:18<00:00,  1.25s/it]\n",
      "[35 / 50] Train: Loss = 0.13533, Accuracy = 94.58%: 100%|██████████| 572/572 [02:54<00:00,  3.93it/s]\n",
      "[35 / 50]   Val: Loss = 0.17119, Accuracy = 93.60%: 100%|██████████| 13/13 [00:17<00:00,  1.33s/it]\n",
      "[36 / 50] Train: Loss = 0.13477, Accuracy = 94.62%: 100%|██████████| 572/572 [02:53<00:00,  3.30it/s]\n",
      "[36 / 50]   Val: Loss = 0.17340, Accuracy = 93.64%: 100%|██████████| 13/13 [00:18<00:00,  1.17s/it]\n",
      "[37 / 50] Train: Loss = 0.13455, Accuracy = 94.62%: 100%|██████████| 572/572 [02:53<00:00,  3.29it/s]\n",
      "[37 / 50]   Val: Loss = 0.17216, Accuracy = 93.36%: 100%|██████████| 13/13 [00:17<00:00,  1.06s/it]\n",
      "[38 / 50] Train: Loss = 0.13435, Accuracy = 94.63%: 100%|██████████| 572/572 [02:54<00:00,  3.29it/s]\n",
      "[38 / 50]   Val: Loss = 0.17228, Accuracy = 93.61%: 100%|██████████| 13/13 [00:18<00:00,  1.43s/it]\n",
      "[39 / 50] Train: Loss = 0.13406, Accuracy = 94.64%: 100%|██████████| 572/572 [02:51<00:00,  4.35it/s]\n",
      "[39 / 50]   Val: Loss = 0.17072, Accuracy = 93.65%: 100%|██████████| 13/13 [00:17<00:00,  1.07s/it]\n",
      "[40 / 50] Train: Loss = 0.13389, Accuracy = 94.61%: 100%|██████████| 572/572 [02:54<00:00,  3.28it/s]\n",
      "[40 / 50]   Val: Loss = 0.17146, Accuracy = 93.63%: 100%|██████████| 13/13 [00:17<00:00,  1.09s/it]\n",
      "[41 / 50] Train: Loss = 0.13347, Accuracy = 94.66%: 100%|██████████| 572/572 [02:52<00:00,  3.32it/s]\n",
      "[41 / 50]   Val: Loss = 0.17001, Accuracy = 93.57%: 100%|██████████| 13/13 [00:18<00:00,  1.42s/it]\n",
      "[42 / 50] Train: Loss = 0.13320, Accuracy = 94.65%: 100%|██████████| 572/572 [02:52<00:00,  3.55it/s]\n",
      "[42 / 50]   Val: Loss = 0.17012, Accuracy = 93.58%: 100%|██████████| 13/13 [00:19<00:00,  1.11s/it]\n",
      "[43 / 50] Train: Loss = 0.13313, Accuracy = 94.64%: 100%|██████████| 572/572 [02:54<00:00,  3.28it/s]\n",
      "[43 / 50]   Val: Loss = 0.16884, Accuracy = 93.63%: 100%|██████████| 13/13 [00:17<00:00,  1.17s/it]\n",
      "[44 / 50] Train: Loss = 0.13257, Accuracy = 94.66%: 100%|██████████| 572/572 [02:53<00:00,  4.06it/s]\n",
      "[44 / 50]   Val: Loss = 0.17283, Accuracy = 93.64%: 100%|██████████| 13/13 [00:18<00:00,  1.31s/it]\n",
      "[45 / 50] Train: Loss = 0.13252, Accuracy = 94.66%: 100%|██████████| 572/572 [02:53<00:00,  3.30it/s]\n",
      "[45 / 50]   Val: Loss = 0.16967, Accuracy = 93.55%: 100%|██████████| 13/13 [00:17<00:00,  1.46s/it]\n",
      "[46 / 50] Train: Loss = 0.13229, Accuracy = 94.66%: 100%|██████████| 572/572 [02:57<00:00,  3.22it/s]\n",
      "[46 / 50]   Val: Loss = 0.16975, Accuracy = 93.59%: 100%|██████████| 13/13 [00:19<00:00,  1.23s/it]\n",
      "[47 / 50] Train: Loss = 0.13186, Accuracy = 94.67%: 100%|██████████| 572/572 [02:52<00:00,  3.32it/s]\n",
      "[47 / 50]   Val: Loss = 0.17051, Accuracy = 93.48%: 100%|██████████| 13/13 [00:19<00:00,  1.23s/it]\n",
      "[48 / 50] Train: Loss = 0.13168, Accuracy = 94.68%: 100%|██████████| 572/572 [02:53<00:00,  3.29it/s]\n",
      "[48 / 50]   Val: Loss = 0.16939, Accuracy = 93.61%: 100%|██████████| 13/13 [00:18<00:00,  1.36s/it]\n",
      "[49 / 50] Train: Loss = 0.13143, Accuracy = 94.69%: 100%|██████████| 572/572 [02:53<00:00,  3.30it/s]\n",
      "[49 / 50]   Val: Loss = 0.16899, Accuracy = 93.57%: 100%|██████████| 13/13 [00:18<00:00,  1.33s/it]\n",
      "[50 / 50] Train: Loss = 0.13135, Accuracy = 94.69%: 100%|██████████| 572/572 [02:53<00:00,  3.30it/s]\n",
      "[50 / 50]   Val: Loss = 0.17054, Accuracy = 93.60%: 100%|██████████| 13/13 [00:18<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab_type": "code",
    "id": "HPUuAPGhEGVR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "e01a5933-993b-40ab-db50-258ee9cfd0a4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.558009144752E12,
     "user_tz": -180.0,
     "elapsed": 29710.0,
     "user": {
      "displayName": "Dissatisfied Сustomer",
      "photoUrl": "",
      "userId": "18374096525467064227"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LSTM with embeddings tagger: 93.63%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct_count = 0\n",
    "sum_count = 0\n",
    "for X_batch_test, y_batch_test in iterate_batches((X_test, y_test), 64):\n",
    "    X_batch_test, y_batch_test = LongTensor(X_batch_test), LongTensor(y_batch_test)\n",
    "    logits = model(X_batch_test)\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    cur_correct_count, cur_sum_count = calc_accuracy(preds, y_batch_test)\n",
    "    correct_count += cur_correct_count\n",
    "    sum_count += cur_sum_count\n",
    "print(\"Accuracy of LSTM with embeddings tagger: {:.2%}\".format( correct_count / sum_count ) )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNNs.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
